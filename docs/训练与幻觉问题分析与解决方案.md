# 训练 13 轮后「生成数量少」与「幻觉」问题分析与解决方案

## 一、现象简述

- **参考报告 (ref)**：完整结构化（所见 / 结论 / 建议 / 病理倾向），长度约 200～400+ 字。
- **生成结果 (gen)**：过短、截断、错字/乱码（如「结芙」「双系」「磷化物」「侬」「窦状」），且常在中途结束。

---

## 二、原因分析

### 1. 生成数量少（输出过短、易截断）

| 原因 | 说明 |
|------|------|
| **max_new_tokens 过小** | 推理默认 256，中文约 150～200 字，难以覆盖完整报告。 |
| **min_new_tokens=64** | 64 步后即可 EOS，模型易过早结束。 |
| **no_repeat_ngram_size=4** | 医学报告里「双肺」「结节」等合理重复被当成重复惩罚，抑制输出长度。 |
| **EOS 抑制仅 64 步** | 前 64 步禁止 EOS 后立刻可结束，不利于长报告。 |
| **训练/推理长度不一致** | 训练时 max_text_len=512，推理 256，模型更易学到「短输出」。 |

### 2. 幻觉（错字、乱码、胡编）

| 原因 | 说明 |
|------|------|
| **Mamba 词表以英文为主** | state-spaces/mamba-2.8b-hf 使用 GPT-2 系 BPE，中文多为子词切分，医学词（如「结节」「磨玻璃」）易被拆成罕见组合，生成时易变成形近错字（结芙、双系等）。 |
| **LLM 完全冻结** | 当前只训练 Vision+Bridge，Mamba 不更新，无法学习中文/医学用词，只能靠 Bridge 把视觉信息压进已有 embedding 空间。 |
| **vocab_size 与 tokenizer 不一致** | 若模型 vocab 大于 tokenizer 实际词数，可能生成无效 token id，解码成乱码（已有 _VocabSizeMaskProcessor 可缓解）。 |

### 3. 训练数据与推理一致性

- 若训练用 `caption_train_struct.csv`（长问题 + 非模板 answer），推理用「所见/结论/建议/病理倾向」模板 prompt，格式不一致会加剧短输出和格式错乱。
- 建议：**训练与验证使用同一套 prompt 与 answer 格式**（例如都用 template CSV）。

---

## 三、已做的代码修改（推理侧）

在 `inference.py` 中已做如下调整，无需改训练即可先验证效果：

1. **max_new_tokens** 默认由 256 改为 **512**，减少报告被截断。
2. **length_penalty** 新增，默认 **1.1**，鼓励更长、完整句子。
3. **no_repeat_ngram_size** 默认改为 **0**（关闭），避免误伤「双肺」「结节」等合理重复。
4. **min_new_tokens** 由 64 改为 **128**，至少多生成一段再允许结束。
5. **_SuppressEOSAtBegin** 的 suppress_steps 由 64 改为 **128**，前 128 步禁止 EOS，减少过早结束。
6. 新增命令行参数：`--length_penalty`、`--no_repeat_ngram_size`、`--suppress_eos_steps`，便于调参。
7. **占位符重复问题（如 run_20260213_224027）**：若 prompt 含「所见：<影像学所见...>」「建议：<随访或检查建议>」等，模型会重复这些占位符而非生成真实报告。已做：(1) 默认改用**无占位符短 prompt**「请根据胸部CT生成报告，按四段输出：所见、结论、建议、病理倾向。」；(2) 后处理过滤仅含占位符的行（如 `建议：<随访或检查建议>`）；(3) 验证 `--val_sample` 默认用该短 prompt，可用 `--use_csv_prompt` 恢复使用 CSV 中的问题。

**建议验证命令示例：**

```bash
python inference.py --val_sample --num_val 4 --max_new_tokens 512
# 若仍偏短可试：
python inference.py --val_sample --max_new_tokens 512 --length_penalty 1.2 --suppress_eos_steps 200
```

---

## 四、训练侧可采取的改进

1. **统一数据格式**  
   - 训练与验证都使用「所见/结论/建议/病理倾向」模板 CSV（如 `caption_train_template.csv` / `caption_val_template.csv`），不要混用 struct 与 template。

2. **保证训练时能看到长报告**  
   - 保持 `max_text_len=512`（或 768），确保长答案不被截断，让模型学习完整四段式结构。

3. **适当增加训练轮数 / 学习率**  
   - 13 轮若 loss 仍明显下降，可试 20～30 轮。  
   - 若幻觉加重，可把 lr 从 2e-5 降到 1e-5。

4. **缓解中文幻觉的根本方向（需额外工作）**  
   - **方案 A**：对 Mamba 做**中文医学语料上的继续预训练**（或 LoRA），让词表/表示更适配中文医学术语。  
   - **方案 B**：换用**支持中文更好的 tokenizer**（如中文 BPE/SPM），并对 embedding 做 resize + 对齐后微调。  
   - **方案 C**：仅微调 LLM 最后几层或加 LoRA（小学习率），在不大幅增加显存的前提下减轻错字。

5. **数据质量**  
   - 用 `scripts/clean_caption_csv.py` 剔除空洞短样本、缺失路径等，减少「学短句、学废话」和图文错位导致的幻觉。

---

## 五、小结

- **数量少 / 截断**：主要通过**推理参数**已做缓解（max_new_tokens=512、length_penalty、关闭 no_repeat_ngram、延长 EOS 抑制）；若仍短可再提高 `--max_new_tokens` 与 `--suppress_eos_steps`。
- **幻觉（错字/乱码）**：根因是 **Mamba 词表与中文医学表达不匹配且 LLM 未参与训练**；短期可依赖推理参数与数据格式统一，中长期需考虑中文/医学继续预训练或 LoRA、或更换/扩展 tokenizer。

重新跑验证示例：

```bash
cd d:\mamba
python inference.py --val_sample --num_val 4
```

查看新生成的 `sample_*_gen.txt` 与参考 `sample_*_ref.txt` 对比长度与错字是否改善。
